{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H4221 - Rapport Machine Learning\n",
    "\n",
    "[a compléter]\n",
    "\n",
    "Auteurs: \n",
    "- DUBILLOT Elise\n",
    "- FLANDRE Corentin\n",
    "- THOMAS Colin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "L'objectif du projet est de tester différents modèles de [a compléter]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "# print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "  \n",
    "dfsorted = df[[\"price\", \"bathrooms\"]]\n",
    "dfsorted.sort_values(by=[\"bathrooms\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"bathrooms\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel('Nombre de salles de bains')\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre de salles de bains')\n",
    "plt.show()\n",
    "  \n",
    "dfsorted = df[[\"price\", \"floors\"]]\n",
    "dfsorted.sort_values(by=[\"floors\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"floors\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Nombre d'étages\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre d\\'étages')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"sqft_living\"]]\n",
    "dfsorted.sort_values(by=[\"sqft_living\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"sqft_living\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Nombre de mètres carrés vivables\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre de mètres carrés vivables')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"bedrooms\"]]\n",
    "dfsorted.sort_values(by=[\"bedrooms\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"bedrooms\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Nombre de chambres\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre de chambres')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"sqft_lot\"]]\n",
    "dfsorted.sort_values(by=[\"sqft_lot\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"sqft_lot\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Taille du terrain en sqrft\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction de la taille du terrain')\n",
    "plt.show()\n",
    "\n",
    "dfsorted = df[[\"price\", \"zipcode\"]]\n",
    "dfsorted.sort_values(by=[\"zipcode\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"zipcode\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"ZIPCODE\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du zipcode')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"lat\"]]\n",
    "dfsorted.sort_values(by=[\"lat\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"lat\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.hist(df[\"lat\"], bins = 30)\n",
    "\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.title('Répartition des données sur la latitude')\n",
    "plt.show()\n",
    "\n",
    "dfsorted = df[[\"price\", \"long\"]]\n",
    "dfsorted.sort_values(by=[\"long\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"long\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.hist(df[\"long\"])\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.title('Répartition des données sur la longitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.density_mapbox(df, lat='lat', lon='long',\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    " \n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque ici plusieurs cas aberrants, que l'on traitera dans la partie traitement des données.\n",
    "Par exemple, dans le cas du nombre de chambre, on remarque une donnée qui représente une maison de 33 chambres, coûtant moins qu'une maison moyenne de 7 chambres et n'ayant que 1.25 salles de bain.\n",
    "On remarque également que le prix et la taille du terrain ne semblent pas avoir un lien au delà de environ 0.05 sqft. On peut supposer que cela est du à la multiplicité de facteurs influençant le prix.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement du jeu de données\n",
    "\n",
    "[blabla à compléter]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au vu des quelques valeurs aberrantes, nous choisissons d'enlever les données qui nous semblent non pertinentes pour nos modèles afin de travailler sur la variable cible du prix (et notamment mieux estimer les prix des prochaines maisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taille du jeu de données initial\n",
    "size_dataset_raws = len(df)\n",
    "index_treatement = []\n",
    "\n",
    "# suppression de la données à plus de 11 chambres\n",
    "print(len(df))\n",
    "# print(f\"before treatment of nb of bedrooms: {len(df.loc[df['bedrooms']==33])}\")\n",
    "indexNames = df[ df['bedrooms'] >= 11 ].index\n",
    "for value in indexNames.values:\n",
    "    index_treatement.append(int(value))\n",
    "# print(f\"I: {index_treatement}\")\n",
    "df.drop(indexNames , inplace=True)\n",
    "# print(f\"after treatment of nb of bedrooms: {len(df.loc[df['bedrooms']==33])}\")\n",
    "\n",
    "# suppression de la données à 13540 m² de surface habitable\n",
    "# print(f\"before treatment of sqft_living: {len(df.loc[df['sqft_living']>=12000])}\")\n",
    "indexNames2 = df[ df['sqft_living'] >= 12000 ].index\n",
    "for value in indexNames2.values:\n",
    "    index_treatement.append(int(value))\n",
    "df.drop(indexNames2 , inplace=True)\n",
    "# print(f\"after treatment of sqft_living: {len(df.loc[df['sqft_living']>=12000])}\")\n",
    "\n",
    "\n",
    "# suppression des données à 7 salles de bains\n",
    "indexNames3 = df[df['bathrooms'] >=7].index\n",
    "for value in indexNames3.values:\n",
    "    index_treatement.append(int(value))\n",
    "df.drop(indexNames3 , inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[traitement densité] <!-- Les données latitude et longitude vont nous permettre de créer une dimension densité, plus utile dans notre cadre de prédiction de prix d'une maison. Il va donc être possible de calculer le nombre de maisons à moins d'une certaine distance. Ce traitement a plus de signification que le traitement sur latitue et longitude: -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un traitement sur la date est à réaliser. Nous faisons le choix d'identifier uniquement le mois de la vente pour l'utilisr plus tard. Le jeu de données prend en compte des dates entre Mai 2014 et Mai 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_dates = []\n",
    "for i in range(size_dataset_raws):\n",
    "    if i not in index_treatement:\n",
    "        year = int(df[\"date\"][i][0:4])\n",
    "        month = int(df[\"date\"][i][4:6])\n",
    "        # day = int(df[\"date\"][i][6:8]) # donnee non utilise pour notre traitement\n",
    "        new_date = 0\n",
    "        if year==2014:\n",
    "            if month==5:\n",
    "                new_date = 1\n",
    "            elif month==6:\n",
    "                new_date = 2    \n",
    "            elif month==7:\n",
    "                new_date = 3 \n",
    "            elif month==8:\n",
    "                new_date = 4 \n",
    "            elif month==9:\n",
    "                new_date = 5 \n",
    "            elif month==10:\n",
    "                new_date = 6 \n",
    "            elif month==11:\n",
    "                new_date = 7\n",
    "            elif month==12:\n",
    "                new_date = 8\n",
    "        elif year==2015:\n",
    "            if month==1:\n",
    "                new_date = 9\n",
    "            elif month==2:\n",
    "                new_date = 10    \n",
    "            elif month==3:\n",
    "                new_date = 11 \n",
    "            elif month==4:\n",
    "                new_date = 12 \n",
    "            elif month==5:\n",
    "                new_date = 13 \n",
    "        new_dates.append(new_date)\n",
    "df.date = new_dates\n",
    "\n",
    "# Gestion des erreurs de dates\n",
    "print(f\"There is {len(df.loc[df['date'] == 0])} raws with date not between May-2014 and May-2015\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au vu de l'étude que l'on veut faire du jeu de données, il faut maintenant ne plus prendre en considération le prix de vente des maisons comme entrées. On les enlève du dataset en le renseignant ailleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de régression linéaire\n",
    "\n",
    "[blabla à compléter]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de régression ridge\n",
    "\n",
    "Dans cette partie, nous allons utiliser un modèle de regression ridge pour prédire les prix d'autres maisons. Pour rappel, il s'agit dans notre cas d'un apprentissage supervisé puisque nous connaissons la sortie des données (c'est à dire le prix) et c'est un modèle linéaire. Nous utiliserons la bibliothèque libre Scikit-Learn.\n",
    "\n",
    "Un mélange des données sera nécessaire, nous l'effectuons de suite. Nous réduisons les dimensions des données aux dimensions que nous jugeons intéressantes pour les données d'entrées de notre algorithme. Nous utilisons la dimension du prix pour les données de sorties.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# début code ridge regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# \"Shuffle\" des données\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "# On décompose le dataset et on le transforme en matrices pour pouvoir effectuer notre calcul\n",
    "X = np.matrix([np.ones(df.shape[0]), df['date'].values, df['sqft_lot'].values ,df['bedrooms'].values, df['bathrooms'].values, df['sqft_living'].values, df['floors'].values, df['waterfront'].values, df['view'].values, df['condition'].values, df['grade'].values, df['sqft_above'].values, df['sqft_basement'].values, df['yr_built'].values, df['yr_renovated'].values,df['zipcode'].values,df['lat'].values,df['long'].values, df['sqft_living15'].values]).T\n",
    "y = np.matrix(df['price']).T\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous normalisons les données d'entrées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir l'efficacité de ce modèle, nous allons diviser notre jeu de données en deux: un \"training test\" et un \"testing set\"\n",
    "Le \"training set\" va permettre d'apprendre pour répondre à notre tâche de prédiction de prix. \n",
    "Le \"testing set\" va permet de mesurer l'erreur de prédiction de prix des maisons sur des données jamais vues par le modèle final. \n",
    "Nous allons repartir notre jeu de données en 80% de données pour le training set et 20% pour le \"testing set\".\n",
    "Il n'y a pas besoin de réduire le jeu d'entrée de données puisque le problème se résout en temps raisonnable sans cette réduction.\n",
    "\n",
    "Le mélange précédent des données permet de biaiser au minimum notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set (80% des valeurs)\n",
    "# Testing set (20% des valeurs)\n",
    "X_training_set, X_testing_set, y_training_set, y_testing_set = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons trouver un **coefficient de régularisation** adapté. Nous appellons ce coefficient alpha, nous allons en tester un certain nombre afin de trouver celui qui est optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 1000\n",
    "alphas = np.logspace(-1, 3.3, n_alphas)\n",
    "R2_alphas = []\n",
    "for i in range(len(alphas)):\n",
    "    clf = Ridge(alpha=alphas[i])\n",
    "    clf.fit(np.asarray(X_training_set), np.asarray(y_training_set)) \n",
    "    y_predict = clf.predict(np.asarray(X_testing_set))\n",
    "    R2_alphas.append(clf.score(np.asarray(X_testing_set),np.asarray(y_testing_set)))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, R2_alphas)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "# ax2 = plt.gca()\n",
    "# ax2.plot(alphas, R2_alphas)\n",
    "# # ax2.set_xscale('log')\n",
    "# plt.xlabel('alpha')\n",
    "# plt.ylabel('R2')\n",
    "# plt.axis('tight')\n",
    "# plt.show()\n",
    "print(f\"Meilleure valeur de alpha: {max(R2_alphas)}\")\n",
    "print(f\"Meilleure valeur de alpha: {alphas[R2_alphas.index(max(R2_alphas))]}\")\n",
    "# print(R2_alphas.)\n",
    "\n",
    "sum_error = 0\n",
    "clf = Ridge(alphas[R2_alphas.index(max(R2_alphas))])\n",
    "clf.fit(np.asarray(X_training_set), np.asarray(y_training_set)) \n",
    "y_predict = clf.predict(np.asarray(X_testing_set))\n",
    "for i in range(len(y_predict)):\n",
    "    sum_error += abs(y_predict[i]-y_testing_set[i])\n",
    "mean_error = sum_error / len(y_predict)\n",
    "print(f\"Moyenne d'écarts d'erreur: {mean_error}\")\n",
    "\n",
    "# fin code ridge regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de régression ridge à noyau\n",
    "\n",
    "[blabla à compléter]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de régression ridge à noyau avec l'approximation de nyström"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "# \"Shuffle\" des données\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "# On décompose le dataset et on le transforme en matrices pour pouvoir effectuer notre calcul\n",
    "X = np.matrix([np.ones(df.shape[0]), df['date'].values, df['sqft_lot'].values ,df['bedrooms'].values, df['bathrooms'].values, df['sqft_living'].values, df['floors'].values, df['waterfront'].values, df['view'].values, df['condition'].values, df['grade'].values, df['sqft_above'].values, df['sqft_basement'].values, df['yr_built'].values, df['yr_renovated'].values,df['zipcode'].values,df['lat'].values,df['long'].values, df['sqft_living15'].values]).T\n",
    "y = np.matrix(df['price']).T\n",
    "\n",
    "\n",
    "X = np.asarray(X)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "\n",
    "# Training set (80% des valeurs)\n",
    "# Testing set (20% des valeurs)\n",
    "X_training_set, X_testing_set, y_training_set, y_testing_set = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "nb_depths = 10\n",
    "depths = []\n",
    "scores = []\n",
    "means = []\n",
    "clocks = []\n",
    "for i in range(nb_depths):\n",
    "    depths.append(i+1)\n",
    "    start = timeit.default_timer()\n",
    "    regr = RandomForestRegressor(max_depth=i+1, random_state=0)\n",
    "    regr.fit(np.asarray(X_training_set), np.asarray(y_training_set))\n",
    "\n",
    "    # Score of the model\n",
    "    scores.append(regr.score(np.asarray(X_testing_set), np.asarray(y_testing_set)))\n",
    "    # print(score)\n",
    "\n",
    "    # moyenne d'écarts\n",
    "    sum_error = 0\n",
    "    y_predict = regr.predict(np.asarray(X_testing_set))\n",
    "    for i in range(len(y_predict)):\n",
    "        sum_error += int(abs(y_predict[i]-y_testing_set[i]))\n",
    "    means.append(sum_error / len(y_predict))\n",
    "    end = timeit.default_timer()\n",
    "    clocks.append(end-start)\n",
    "ax = plt.gca()\n",
    "ax.plot(depths, scores)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('R2')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(depths, means)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('mean error')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(depths, clocks)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('time (s)')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Avec la plus grande profondeur ({nb_depths}) : \\nR2 = {scores[nb_depths-1]}\\nmean_error = {means[nb_depths-1]}$\\ntime_learning = {clocks[nb_depths-1]}s\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
