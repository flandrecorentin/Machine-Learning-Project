{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H4221 - Rapport Machine Learning\n",
    "\n",
    "Au cours du TP de Machine Learning, nous allons entrainer de multiples modèles/algorithme de Machine Learning sur un jeu de données sur les ventes de maisons à Seattle (USA) afin de pouvoir prédire les prix d'autres maisons. \n",
    "\n",
    "Auteurs: \n",
    "- DUBILLOT Elise\n",
    "- FLANDRE Corentin\n",
    "- THOMAS Colin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Le TP de Machine Learning va se diviser en plusieurs étapes clés: \n",
    "- Récupération de données\n",
    "- Nettoyage du jeu de données\n",
    "- Exploration du jeu de données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération du jeu de données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre jeu de données est disponible à l'url suivante : https://www.kaggle.com/datasets/harlfoxem/housesalesprediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"kc_house_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'avère que le jeu de données est très large et propre qui comporte des mesures numériques (hormis la date qui sera géré plus tard). \n",
    "Du fait du jeu de données propre, aucun nettoyage n'est nécessaire.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "  \n",
    "dfsorted = df[[\"price\", \"bathrooms\"]]\n",
    "dfsorted.sort_values(by=[\"bathrooms\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"bathrooms\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel('Nombre de salles de bains')\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre de salles de bains')\n",
    "plt.show()\n",
    "  \n",
    "dfsorted = df[[\"price\", \"floors\"]]\n",
    "dfsorted.sort_values(by=[\"floors\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"floors\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Nombre d'étages\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre d\\'étages')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"sqft_living\"]]\n",
    "dfsorted.sort_values(by=[\"sqft_living\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"sqft_living\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Nombre de mètres carrés vivables\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre de mètres carrés vivables')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"bedrooms\"]]\n",
    "dfsorted.sort_values(by=[\"bedrooms\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"bedrooms\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Nombre de chambres\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du nombre de chambres')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"sqft_lot\"]]\n",
    "dfsorted.sort_values(by=[\"sqft_lot\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"sqft_lot\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"Taille du terrain en sqrft\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction de la taille du terrain')\n",
    "plt.show()\n",
    "\n",
    "dfsorted = df[[\"price\", \"zipcode\"]]\n",
    "dfsorted.sort_values(by=[\"zipcode\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"zipcode\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.plot(dfpbyb[:])\n",
    "\n",
    "plt.xlabel(\"ZIPCODE\")\n",
    "plt.ylabel('Prix moyen (en $)')\n",
    "plt.title('Prix en fonction du zipcode')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "dfsorted = df[[\"price\", \"lat\"]]\n",
    "dfsorted.sort_values(by=[\"lat\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"lat\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.hist(df[\"lat\"], bins = 30)\n",
    "\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.title('Répartition des données sur la latitude')\n",
    "plt.show()\n",
    "\n",
    "dfsorted = df[[\"price\", \"long\"]]\n",
    "dfsorted.sort_values(by=[\"long\"], inplace=True)\n",
    "\n",
    "dfpbyb = dfsorted.groupby(\"long\")[\"price\"].mean()\n",
    "\n",
    "dfpbyb.to_numpy()\n",
    "plt.hist(df[\"long\"])\n",
    "\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.title('Répartition des données sur la longitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly.express as px\n",
    "\n",
    "#fig = px.density_mapbox(df, lat='lat', lon='long',\n",
    "#                        mapbox_style=\"stamen-terrain\")\n",
    " \n",
    "#fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque ici plusieurs cas aberrants, que l'on traitera dans la partie traitement des données.\n",
    "Par exemple, dans le cas du nombre de chambre, on remarque une donnée qui représente une maison de 33 chambres, coûtant moins qu'une maison moyenne de 7 chambres et n'ayant que 1.25 salles de bain.\n",
    "On remarque également que le prix et la taille du terrain ne semblent pas avoir un lien au delà de environ 0.05 sqft. On peut supposer que cela est du à la multiplicité de facteurs influençant le prix.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement du jeu de données\n",
    "\n",
    "Suite à la phase d'exploration de données, nous allons les traiter afin d'avoir de meilleurs modèles. Ce traitement concerne principalement les valeurs aberrantes. Ces données abberrantes ne semblent pas pertinentes pour nos modèles afin de mieux travailler sur la variable cible du prix (et mieux estimer les prix des prochaines maisons). L'étape de traitement du jeu de données concerne aussi les dates qui seront traités afin de devenir une donnée numérique. Il aurait été possible d'utiliser un \"encodage one-hot\" mais  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taille du jeu de données initial\n",
    "size_dataset_raws = len(df)\n",
    "index_treatement = []\n",
    "\n",
    "# suppression de la données à plus de 11 chambres\n",
    "# print(f\"before treatment of nb of bedrooms: {len(df.loc[df['bedrooms']==33])}\")\n",
    "indexNames = df[ df['bedrooms'] >= 11 ].index\n",
    "for value in indexNames.values:\n",
    "    index_treatement.append(int(value))\n",
    "# print(f\"I: {index_treatement}\")\n",
    "df.drop(indexNames , inplace=True)\n",
    "# print(f\"after treatment of nb of bedrooms: {len(df.loc[df['bedrooms']==33])}\")\n",
    "\n",
    "# suppression de la données à 13540 m² de surface habitable\n",
    "# print(f\"before treatment of sqft_living: {len(df.loc[df['sqft_living']>=12000])}\")\n",
    "indexNames2 = df[ df['sqft_living'] >= 12000 ].index\n",
    "for value in indexNames2.values:\n",
    "    index_treatement.append(int(value))\n",
    "df.drop(indexNames2 , inplace=True)\n",
    "# print(f\"after treatment of sqft_living: {len(df.loc[df['sqft_living']>=12000])}\")\n",
    "\n",
    "\n",
    "# suppression des données à 7 salles de bains\n",
    "indexNames3 = df[df['bathrooms'] >=7].index\n",
    "for value in indexNames3.values:\n",
    "    index_treatement.append(int(value))\n",
    "df.drop(indexNames3 , inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce qui est des dates, nous faisons le choix d'identifier uniquement le numéro de mois. Le jeu de données prend en compte des dates entre Mai 2014 et Mai 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_dates = []\n",
    "for i in range(size_dataset_raws):\n",
    "    if i not in index_treatement:\n",
    "        year = int(df[\"date\"][i][0:4])\n",
    "        month = int(df[\"date\"][i][4:6])\n",
    "        # day = int(df[\"date\"][i][6:8]) # donnee non utilise pour notre traitement\n",
    "        new_date = 0\n",
    "        if year==2014:\n",
    "            if month==5:\n",
    "                new_date = 1\n",
    "            elif month==6:\n",
    "                new_date = 2    \n",
    "            elif month==7:\n",
    "                new_date = 3 \n",
    "            elif month==8:\n",
    "                new_date = 4 \n",
    "            elif month==9:\n",
    "                new_date = 5 \n",
    "            elif month==10:\n",
    "                new_date = 6 \n",
    "            elif month==11:\n",
    "                new_date = 7\n",
    "            elif month==12:\n",
    "                new_date = 8\n",
    "        elif year==2015:\n",
    "            if month==1:\n",
    "                new_date = 9\n",
    "            elif month==2:\n",
    "                new_date = 10    \n",
    "            elif month==3:\n",
    "                new_date = 11 \n",
    "            elif month==4:\n",
    "                new_date = 12 \n",
    "            elif month==5:\n",
    "                new_date = 13 \n",
    "        new_dates.append(new_date)\n",
    "df.date = new_dates\n",
    "\n",
    "# Gestion des erreurs de dates\n",
    "print(f\"There is {len(df.loc[df['date'] == 0])} raws with date not between May-2014 and May-2015\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de régression linéaire\n",
    "\n",
    "[blabla à compléter]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de régression ridge\n",
    "\n",
    "Dans cette partie, nous allons utiliser un modèle de regression ridge pour prédire les prix d'autres maisons. Pour rappel, il s'agit dans notre cas d'un apprentissage supervisé puisque nous connaissons la sortie des données (c'est à dire le prix) et c'est un modèle linéaire paramétrique. Nous utiliserons la bibliothèque libre Scikit-Learn.\n",
    "\n",
    "Un mélange des données sera nécessaire, nous l'effectuons de suite. Nous réduisons les dimensions des données aux dimensions que nous jugeons intéressantes pour les données d'entrées de notre algorithme (en utilisant l'étape d'exploration). Nous utilisons la dimension du prix pour les données de sorties.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# début code ridge regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# \"Shuffle\" des données\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "# On décompose le dataset et on le transforme en matrices pour pouvoir effectuer notre calcul\n",
    "X = np.matrix([np.ones(df.shape[0]), df['date'].values, df['sqft_lot'].values ,df['bedrooms'].values, df['bathrooms'].values, df['sqft_living'].values, df['floors'].values, df['waterfront'].values, df['view'].values, df['condition'].values, df['grade'].values, df['sqft_above'].values, df['sqft_basement'].values, df['yr_built'].values, df['yr_renovated'].values,df['zipcode'].values,df['lat'].values,df['long'].values, df['sqft_living15'].values]).T\n",
    "y = np.matrix(df['price']).T\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous normalisons les données d'entrées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir l'efficacité de ce modèle, nous allons diviser notre jeu de données en deux: un \"training test\" et un \"testing set\"\n",
    "Le \"training set\" va permettre d'apprendre pour répondre à notre tâche de prédiction de prix. \n",
    "Le \"testing set\" va permet de mesurer l'erreur de prédiction de prix des maisons sur des données jamais vues par le modèle final. \n",
    "Nous allons repartir notre jeu de données en 80% de données pour le training set et 20% pour le \"testing set\".\n",
    "Il n'y a pas besoin de réduire le jeu d'entrée de données puisque le problème se résout en temps raisonnable sans cette réduction.\n",
    "\n",
    "Le mélange précédent des données permet de biaiser au minimum notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set (80% des valeurs)\n",
    "# Testing set (20% des valeurs)\n",
    "X_training_set, X_testing_set, y_training_set, y_testing_set = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons trouver un **coefficient de régularisation** adapté. Nous appellons ce coefficient alpha, nous allons en tester un certain nombre afin de trouver celui qui est optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 1000\n",
    "alphas = np.logspace(-1, 3.3, n_alphas)\n",
    "R2_alphas = []\n",
    "for i in range(len(alphas)):\n",
    "    clf = Ridge(alpha=alphas[i])\n",
    "    clf.fit(np.asarray(X_training_set), np.asarray(y_training_set)) \n",
    "    y_predict = clf.predict(np.asarray(X_testing_set))\n",
    "    R2_alphas.append(clf.score(np.asarray(X_testing_set),np.asarray(y_testing_set)))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, R2_alphas)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "# ax2 = plt.gca()\n",
    "# ax2.plot(alphas, R2_alphas)\n",
    "# # ax2.set_xscale('log')\n",
    "# plt.xlabel('alpha')\n",
    "# plt.ylabel('R2')\n",
    "# plt.axis('tight')\n",
    "# plt.show()\n",
    "print(f\"Meilleure valeur de alpha: {max(R2_alphas)}\")\n",
    "print(f\"Meilleure valeur de alpha: {alphas[R2_alphas.index(max(R2_alphas))]}\")\n",
    "# print(R2_alphas.)\n",
    "\n",
    "sum_error = 0\n",
    "clf = Ridge(alphas[R2_alphas.index(max(R2_alphas))])\n",
    "clf.fit(np.asarray(X_training_set), np.asarray(y_training_set)) \n",
    "y_predict = clf.predict(np.asarray(X_testing_set))\n",
    "for i in range(len(y_predict)):\n",
    "    sum_error += abs(y_predict[i]-y_testing_set[i])\n",
    "mean_error = sum_error / len(y_predict)\n",
    "print(f\"Moyenne d'écarts d'erreur: {mean_error}\")\n",
    "\n",
    "# fin code ridge regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au vu du modèle de ridge regression, il existe bien un alpha qui maximise le coefficient de détermination. Cette valeur de alpha permet d'avoir un modèle avec un compromis entre biais et variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de régression ridge à noyau\n",
    "\n",
    "Nous effectuons une Régression Ridge à noyau Gaussien."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous séparons tout d'abord un training set et un testing set. Nous décidons ici de réduire la taille de nos training et testing set, car les temps de calculs étaient trop long. \n",
    "Il sera envisageable de commenter les lignes correspondantes à la réduction de taille des set et de laisser tourner le programme pendant toute une journée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Shuffle des données\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "# On décompose le dataset et on le transforme en matrices pour pouvoir effectuer notre calcul\n",
    "X = np.matrix([np.ones(df.shape[0]), df['date'].values,  df['bedrooms'].values, df['bathrooms'].values, df['sqft_living'].values, df['floors'].values, df['waterfront'].values, df['view'].values, df['condition'].values,\n",
    "              df['grade'].values, df['sqft_above'].values, df['sqft_basement'].values, df['yr_built'].values, df['yr_renovated'].values, df['zipcode'].values, df['lat'].values, df['long'].values, df['sqft_living15'].values]).T\n",
    "y = np.matrix(df['price']).T\n",
    "\n",
    "\n",
    "# Training set (80% des valeurs)\n",
    "# Testing set (20% des valeurs)\n",
    "X_training_set, X_testing_set, y_training_set, y_testing_set = train_test_split(\n",
    "    X, y, train_size=0.8)\n",
    "\n",
    "X_training_set = X_training_set[1:4000]\n",
    "y_training_set = y_training_set[1:4000]\n",
    "\n",
    "\n",
    "X_testing_set = X_testing_set[1:1000]\n",
    "y_testing_set = y_testing_set[1:1000]\n",
    "\n",
    "\n",
    "X_training_set_norm = np.asarray(X_training_set)\n",
    "scaler = preprocessing.StandardScaler().fit(X_training_set_norm)\n",
    "X_training_set_norm = scaler.transform(X_training_set_norm)\n",
    "X_training_set = np.asmatrix(X_training_set_norm)\n",
    "\n",
    "\n",
    "X_testing_set_norm = np.asarray(X_testing_set)\n",
    "scaler = preprocessing.StandardScaler().fit(X_testing_set_norm)\n",
    "X_testing_set_norm = scaler.transform(X_testing_set_norm)\n",
    "X_testing_set = np.asmatrix(X_testing_set_norm)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous décidons de régler nos hyper-paramètres. Dans un premier temps, nous allons essayer de les trouver par essais, en utilisant la cross-validation. Nous trouvons d'abord le meilleur Gamma, en fixant Alpha à 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "n_gammas = 10\n",
    "gammas = np.logspace(-4, 1, n_gammas)\n",
    "R2_gammas = []\n",
    "for i in range(len(gammas)):\n",
    "   \n",
    "    clf = KernelRidge(alpha=1, kernel=\"rbf\", gamma = gammas[i])\n",
    "    scores = cross_val_score(clf, np.asarray(X_training_set), np.asarray(y_training_set), cv=5)\n",
    "    print(scores)\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "    R2_gammas.append(scores.mean())\n",
    "\n",
    "\n",
    "for i in range(len(gammas)):\n",
    "    print(f\"{gammas[i]}: {R2_gammas[i]}\")\n",
    "ax = plt.gca()\n",
    "ax.plot(gammas, R2_gammas)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('R2')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "# fin code ridge regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "maximum = max(R2_gammas)\n",
    "GammaMax = gammas[R2_gammas.index(maximum)]\n",
    "print(\"MAX : \"+str(maximum)+\", at : \"+str(GammaMax))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous réglons maintenant Alpha, en utilisant le Gamma optimal trouvé à l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 10\n",
    "alphas = np.logspace(-7, 1, n_alphas)\n",
    "R2_alphas = []\n",
    "for i in range(len(alphas)):\n",
    "    clf = KernelRidge(alpha=alphas[i], kernel=\"rbf\", gamma = GammaMax)\n",
    "    scores = cross_val_score(clf, np.asarray(X_training_set), np.asarray(y_training_set), cv=5)\n",
    "    print(scores)\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "    R2_alphas.append(scores.mean())\n",
    "\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    print(f\"{alphas[i]}: {R2_alphas[i]}\")\n",
    "print(max(R2_alphas))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, R2_alphas)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R2')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "maximum = max(R2_alphas)\n",
    "valuemax = alphas[R2_alphas.index(maximum)]\n",
    "print(\"MAXIMUM : \"+str(maximum)+\", at : \"+str(valuemax))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = KernelRidge(alpha=valuemax, kernel=\"rbf\", gamma = GammaMax)\n",
    "scores = cross_val_score(clf, np.asarray(X_training_set), np.asarray(y_training_set), cv=5)\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "R2_alphas.append(scores.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous trouvons ici une précision de 0.82."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous décidons, maintenant que nous avons un ordre d'idée des hyper-paramètres optimaux, de réaliser une GridSearch plus précise sur le jeu de données, en parcourant les paramètres Alpha et Gamma, avec une 5-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "n_gammas = 3\n",
    "gammas = GammaMax*np.logspace(-0.1, +0.1, n_gammas)\n",
    "n_alphas = 3\n",
    "alphas = valuemax*np.logspace(-0.1, +0.1, n_alphas)\n",
    "parameters = {'gamma': gammas, 'alpha':alphas}\n",
    "grid = GridSearchCV(clf, parameters, verbose=2, return_train_score=True)\n",
    "grid.fit(np.asarray(X_training_set), np.asarray(y_training_set))\n",
    "grid.best_params_\n",
    "\n",
    "clf = KernelRidge(alpha=grid.best_params_['alpha'], kernel=\"rbf\", gamma = grid.best_params_['gamma'])\n",
    "scores = cross_val_score(clf, np.asarray(X_training_set), np.asarray(y_training_set), cv=5)\n",
    "print(scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons pu réaliser ici, avec une Régression Ridge à noyau Gaussien, faire une première recherche d'ordre de grandeur des hyper-paramètres Alpha et Gamma un à un, puis effectuer une recherche plus précise, en effectuant une GridSearch. Nous avons calculés tous les résultats avec une 5-fold Cross-Validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle Random Forest\n",
    "\n",
    "Idem, que précédemment, nous allons appliquer le modèle Random Forest (non vu en cours). Nous allons essayer d'avoir un modèle performant en analysant le résultat avec différentes profondeurs d'algorithmes. Nous analyserons le coefficient de détermination, la moyenne d'erreur sur le training test et le temps d'exécution pour chaque profondeur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from datetime import datetime\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "# \"Shuffle\" des données\n",
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "# On décompose le dataset et on le transforme en matrices pour pouvoir effectuer notre calcul\n",
    "X = np.matrix([np.ones(df.shape[0]), df['date'].values, df['sqft_lot'].values ,df['bedrooms'].values, df['bathrooms'].values, df['sqft_living'].values, df['floors'].values, df['waterfront'].values, df['view'].values, df['condition'].values, df['grade'].values, df['sqft_above'].values, df['sqft_basement'].values, df['yr_built'].values, df['yr_renovated'].values,df['zipcode'].values,df['lat'].values,df['long'].values, df['sqft_living15'].values]).T\n",
    "y = np.matrix(df['price']).T\n",
    "\n",
    "\n",
    "X = np.asarray(X)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "\n",
    "# Training set (80% des valeurs)\n",
    "# Testing set (20% des valeurs)\n",
    "X_training_set, X_testing_set, y_training_set, y_testing_set = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "nb_depths = 15\n",
    "depths = []\n",
    "scores = []\n",
    "means = []\n",
    "clocks = []\n",
    "for i in range(nb_depths):\n",
    "    depths.append(i+1)\n",
    "    start = timeit.default_timer()\n",
    "    regr = RandomForestRegressor(max_depth=i+1, random_state=0)\n",
    "    regr.fit(np.asarray(X_training_set), np.asarray(y_training_set))\n",
    "\n",
    "    # Score of the model\n",
    "    scores.append(regr.score(np.asarray(X_testing_set), np.asarray(y_testing_set)))\n",
    "    # print(score)\n",
    "\n",
    "    # moyenne d'écarts\n",
    "    sum_error = 0\n",
    "    y_predict = regr.predict(np.asarray(X_testing_set))\n",
    "    for i in range(len(y_predict)):\n",
    "        sum_error += int(abs(y_predict[i]-y_testing_set[i]))\n",
    "    means.append(sum_error / len(y_predict))\n",
    "    end = timeit.default_timer()\n",
    "    clocks.append(end-start)\n",
    "ax = plt.gca()\n",
    "ax.plot(depths, scores)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('R2')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(depths, means)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('mean error')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(depths, clocks)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('time (s)')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Avec la plus grande profondeur ({nb_depths}) : \\nR2 = {scores[nb_depths-1]}\\nmean_error = {means[nb_depths-1]}$\\ntime_learning = {clocks[nb_depths-1]}s\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De toute évidence, plus la profondeur de l'algorithme Random Forest permet d'avoir une meilleure prédiction dans notre cas. On arrive à un coefficient de détermination acceptable. Un grand avantage de cette algorithme c'est qu'il est linéaire en temps d'apprentissage. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Pour conclure, durant ce TP de Machine Learning de deux séances, nous avons appliquer une démarche scientifique en datascience (récupération, nettoyage, exploration, modélisation, évaluation et interprétation). Nous avons aussi appris à utiliser différents algorithmes d'apprentissage (apprentissage supervisé) appliqués pour de la régression. Il a été ainsi possible de faire de la prédiction. Parmi les modèles appliqués, nous avons travaillé sur des modèles paramétriques et non-paramétriques. Nous avons aussi utiliser la méthode de Cross-Validation dans le cas du modèle de Ridge Regression.  \n",
    "\n",
    "Pour ce qui est des outils utilisés pour ce TP de Machine Learning, nous avons appris à manier les notebooks Python (Jupyter). Nous avons aussi appris à utiliser les bibliothèques Python de l'écosystème Spicy dont pandas (pour tableaux et Dataframes), numpy (matrice), matplotlib (pour les graphes), iPython (feuilles de calcul). La bibliothèque pour utiliser les algorithmes est Scikit-learn (même si l'outil Tensorflow est omniprésent dans le monde du Machine Learning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
